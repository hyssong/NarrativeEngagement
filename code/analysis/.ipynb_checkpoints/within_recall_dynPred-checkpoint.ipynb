{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall fidelity within-dataset prediction\n",
    "\n",
    "Song, Finn, & Rosenberg (2021) Neural signatures of attentional engagement during narratives and its consequences for event memory, bioRxiv, doi.org/10.1101/2020.08.26.266320\n",
    "\n",
    "Code by Hayoung Song (hyssong@uchicago.edu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, linalg\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "def conv_r2z(r):\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        return 0.5 * (np.log(1 + r) - np.log(1 - r))\n",
    "def conv_z2r(z):\n",
    "    with np.errstate(invalid='ignore', divide='ignore'):\n",
    "        return (np.exp(2 * z) - 1) / (np.exp(2 * z) + 1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ------------------ select dataset ------------------ ##\n",
    "dataset = 'sherlock'\n",
    "thres = 0.01\n",
    "nR = 122\n",
    "path = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "\n",
    "nsubj = scipy.io.loadmat(path+'/data/hyperparameters.mat')[dataset + '_nsubj'][0][0]\n",
    "wsize = scipy.io.loadmat(path+'/data/hyperparameters.mat')[dataset + '_wsize'][0]\n",
    "nT = scipy.io.loadmat(path+'/data/hyperparameters.mat')[dataset + '_nT'][0][0]\n",
    "\n",
    "print('Within-dataset recall prediction')\n",
    "print('  dataset = '+str(dataset))\n",
    "print('  nsubj   = '+str(nsubj))\n",
    "print('  nregion = '+str(nR))\n",
    "print('  thres   = '+str(thres))\n",
    "print('  wsize   = '+str(wsize))\n",
    "print('  network = '+brainnetwork)\n",
    "\n",
    "## ------------------ select wsize option ------------------ ##\n",
    "wsize = wsize[1]\n",
    "print('  selected wsize   = '+str(wsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load recall data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset=='paranoia':\n",
    "    recall = scipy.io.loadmat(path + '/data_processed/' + dataset + '/win'+str(wsize) + '/sliding-recall.mat')['sliding_recall']\n",
    "    recall_surr = scipy.io.loadmat(path + '/data_processed/' + dataset + '/win'+str(wsize) + '/sliding-recall-surr.mat')['sliding_surr_recall']\n",
    "elif dataset=='sherlock':\n",
    "    recall = scipy.io.loadmat(path + '/data_processed/' + dataset + '/win'+str(wsize) + '/sliding-machine-recall.mat')['sliding_recall']\n",
    "    recall_surr = scipy.io.loadmat(path + '/data_processed/' + dataset + '/win'+str(wsize) + '/sliding-recall-machine-surr.mat')['sliding_surr_recall']\n",
    "plt.plot(recall[:,:5])\n",
    "plt.title(dataset+' recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load brain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynFeat = scipy.io.loadmat(path + '/data_processed/' + dataset + '/win'+str(wsize) + '/sliding-dynFeat.mat')['dynFeat']\n",
    "dynFeat = scipy.stats.zscore(dynFeat,2,nan_policy='omit') # zscore per feature\n",
    "print(dynFeat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network mask\n",
    "We're going to use selective functional connections that are correlated with engagement. In a within-dataset prediction (leave one subject out cross-validations), we selected functional connections that significantly correlates with group-average engagement (one-sample t-test, p < thres, thres=0.01). Here, we use functional connections that were selected in *every* round of cross-validations.\n",
    "This code runs after the output from within-dataset prediction is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrative engagement network mask\n",
    "engagement_pos_feat = np.mean(scipy.io.loadmat(path+'/result/dynPred/'+dataset+'/win'+str(wsize)+'/within_engagement.mat')['pos_feat'],0)\n",
    "engagement_neg_feat = np.mean(scipy.io.loadmat(path+'/result/dynPred/'+dataset+'/win'+str(wsize)+We're going to use selective functional connections that are correlated with engagement. In a within-dataset prediction (leave one subject out cross-validations), we selected functional connections that significantly correlates with group-average engagement (one-sample t-test, p < thres, thres=0.01). Here, we use functional connections that were selected in *every* round of cross-validations.\n",
    "This code runs after the output from within-dataset prediction is saved.'/within_engagement.mat')['neg_feat'], 0)\n",
    "for i1 in range(nR):\n",
    "    for i2 in range(nR):\n",
    "        if engagement_pos_feat[i1,i2]==1:\n",
    "            pass\n",
    "        else:\n",
    "            engagement_pos_feat[i1,i2]=0\n",
    "for i1 in range(nR):\n",
    "    for i2 in range(nR):\n",
    "        if engagement_neg_feat[i1, i2] == 1:\n",
    "            pass\n",
    "        else:\n",
    "            engagement_neg_feat[i1, i2] = 0\n",
    "\n",
    "print('Engagement network: #pos = ' + str(int(np.sum(engagement_pos_feat) / 2)), ', #neg = ' + str(int(np.sum(engagement_neg_feat) / 2)))\n",
    "all_feat = engagement_pos_feat + engagement_neg_feat\n",
    "\n",
    "featid = []\n",
    "ii = -1\n",
    "for i1 in range(nR-1):\n",
    "    for i2 in range(i1+1,nR):\n",
    "        ii=ii+1\n",
    "        if all_feat[i1,i2]==1:\n",
    "            featid.append(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynFeat = dynFeat[:,featid,:]\n",
    "nanidx = []\n",
    "for subj in range(dynFeat.shape[0]):\n",
    "    for ft in range(dynFeat.shape[1]):\n",
    "        if dataset=='sherlock' and subj==4:\n",
    "            if np.any(np.isnan(dynFeat[subj,ft,:nT-wsize-51])):\n",
    "                nanidx.append(ft)\n",
    "        else:\n",
    "            if np.any(np.isnan(dynFeat[subj,ft,:])):\n",
    "                nanidx.append(ft)\n",
    "nanidx = np.unique(nanidx)\n",
    "if len(nanidx)>0:\n",
    "    dynFeat = np.delete(dynFeat,nanidx,1)\n",
    "print(str(dynFeat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-subject-out cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def losocv_noftselect(brainfeat, behavior, nsubj):\n",
    "    output_acc, output_eval = [], []\n",
    "    for test_sub in range(nsubj):\n",
    "\n",
    "        # separate train-test subject\n",
    "        test_data = brainfeat[test_sub,:,:]\n",
    "        train_data = np.delete(brainfeat,test_sub,0)\n",
    "        test_behavior = behavior[test_sub,:]\n",
    "        train_behavior = np.delete(behavior, test_sub,0)\n",
    "\n",
    "        # every training participants' data are concatenated\n",
    "        # fed as independent instances to the model\n",
    "        train_feat = np.transpose(train_data,(1,0,2))\n",
    "        train_feat = np.reshape(train_feat,(train_feat.shape[0],train_feat.shape[1]*train_feat.shape[2]))\n",
    "        train_behavior = np.reshape(train_behavior,(train_behavior.shape[0]*train_behavior.shape[1]))\n",
    "\n",
    "        # if several TRs are removed\n",
    "        rmtr_train = []\n",
    "        for tm in range(train_feat.shape[1]):\n",
    "            if np.all(np.isnan(train_feat[:, tm])):\n",
    "                rmtr_train.append(tm)\n",
    "        rmtr_train = np.asarray(rmtr_train)\n",
    "        if len(rmtr_train) > 0:\n",
    "            train_feat = np.delete(train_feat, rmtr_train, 1)\n",
    "            train_behavior = np.delete(train_behavior, rmtr_train, 0)\n",
    "        rmtr_test = []\n",
    "        for tm in range(test_data.shape[1]):\n",
    "            if np.all(np.isnan(test_data[:,tm])):\n",
    "                rmtr_test.append(tm)\n",
    "        if len(rmtr_test) > 0:\n",
    "            test_data = np.delete(test_data,rmtr_test,1)\n",
    "            test_behavior = np.delete(test_behavior, rmtr_test,0)\n",
    "\n",
    "        # Support vector regression with non-linear kernel\n",
    "        clf = []\n",
    "        clf = svm.SVR(kernel='rbf',max_iter=1000, gamma='auto')\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "            clf.fit(train_feat.T, train_behavior)\n",
    "        predicted = clf.predict(test_data.T)\n",
    "        output_acc.append(predicted)\n",
    "\n",
    "        # evaluate\n",
    "        pearsonr = scipy.stats.pearsonr(test_behavior, predicted)\n",
    "        mse = metrics.mean_squared_error(test_behavior, predicted)\n",
    "        rsq = metrics.r2_score(test_behavior, predicted)\n",
    "        output_eval.append([pearsonr[0], mse, rsq])\n",
    "        print('  subj'+str(test_sub+1)+': train='+str(train_feat.shape[1])+', test='+str(test_data.shape[1])+': pearson r='+str(np.round(pearsonr[0],3))+\n",
    "              ', mse='+str(np.round(mse,3))+', rsq='+str(np.round(rsq,3)))\n",
    "\n",
    "    output_acc, output_eval = np.asarray(output_acc), np.asarray(output_eval)\n",
    "    return output_acc, output_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_acc, real_eval = losocv_noftselect(dynFeat, recall.T, nsubj)\n",
    "print('real acc = '+str(np.mean(real_eval[:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath=path+'/result/dynPred/'+str(dataset)+'/win'+str(wsize)\n",
    "result = {'acc':real_acc, 'eval':real_eval}\n",
    "if os.path.exists(savepath)==0:\n",
    "    os.makedirs(savepath)\n",
    "scipy.io.savemat(savepath+'/within_recall_'+brainnetwork+'.mat',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric permutation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(savepath+'/within_recall_'+brainnetwork+'_null'):\n",
    "    os.makedirs(savepath+'/within_recall_'+brainnetwork+'_null')\n",
    "\n",
    "niter = 5\n",
    "surr_pearsonr = []\n",
    "for surr in range(niter):\n",
    "    if os.path.exists(savepath+'/within_recall_'+brainnetwork+'_null/null'+str(surr+1)+'.mat')==0:\n",
    "        print('null ' + str(surr + 1) + ' / ' + str(niter))\n",
    "        \n",
    "        recall = recall_surr[:,:,surr]\n",
    "        surr_acc, surr_eval = losocv_noftselect(dynFeat, recall.T, nsubj)\n",
    "        \n",
    "        print('null ' + str(it + 1) +': acc'+str(surr+1)+' = ' + str(np.mean(surr_eval[:, 0])))\n",
    "        surr_pearsonr.append(surr_eval[:,0])\n",
    "        result = {'acc':surr_acc, 'eval':surr_eval}\n",
    "        scipy.io.savemat(savepath+'/within_recall_'+brainnetwork+'_null/null'+str(surr+1)+'.mat',result)\n",
    "surr_pearsonr = np.asarray(surr_pearsonr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare an actual prediction accuracy with null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onetail_p(real, null):\n",
    "    p = (1 + np.sum(null>=real)) / (1+len(null))\n",
    "    print(str(np.sum(null>=real))+' among '+str(len(null))+' null has higher r value than actual prediction')\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = onetail_p(np.average(real_eval[:,0]), np.average(surr_pearsonr,1))\n",
    "print('p = '+str(np.round(p,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(surr_pearsonr,1))\n",
    "plt.hist(np.average(surr_pearsonr,1))\n",
    "plt.vlines(np.mean(real_eval[:,0]),0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
